import asyncio
import json
import re
from typing import List, Dict, Tuple
from database.crud import get_prompts
from services.ai_service import analyze_comments_with_prompt
import numpy as np
from dataclasses import dataclass
from datetime import datetime


@dataclass
class IdeaEvaluation:
    idea: str
    scores: Dict[str, float]  # –∫—Ä–∏—Ç–µ—Ä–∏–π -> –æ—Ü–µ–Ω–∫–∞
    feedback: str
    ai_type: str  # creative/analytical/practical


@dataclass
class OptimizedIdea:
    original_idea: str
    optimized_idea: str
    iteration: int
    final_score: float
    confidence: float  # –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ (0-1)
    evaluation_history: List[IdeaEvaluation]


class AdvancedIterativeIdeasOptimizer:
    def __init__(self):
        self.evaluation_criteria = {
            "engagement": {"weight": 0.25, "min": 1, "max": 10},
            "viral_potential": {"weight": 0.20, "min": 1, "max": 10},
            "uniqueness": {"weight": 0.15, "min": 1, "max": 10},
            "production_cost": {"weight": 0.15, "min": 1, "max": 10, "inverted": True},
            "audience_fit": {"weight": 0.15, "min": 1, "max": 10},
            "trend_relevance": {"weight": 0.10, "min": 1, "max": 10}
        }
        
        self.ai_weights = {
            "creative_ai": 0.35,
            "analytical_ai": 0.35, 
            "practical_ai": 0.30
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.parsing_config = {
            "engagement": ["engagement", "–≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å", "involvement", "involucramiento"],
            "viral_potential": ["viral", "–≤–∏—Ä–∞–ª—å–Ω—ã–π", "viralidad", "sharable"],
            "uniqueness": ["uniqueness", "—É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å", "unicidad", "originality"],
            "production_cost": ["production", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "costo", "cost", "—Ä–µ—Å—É—Ä—Å—ã"],
            "audience_fit": ["audience", "–∞—É–¥–∏—Ç–æ—Ä–∏—è", "audiencia", "publico"],
            "trend_relevance": ["trend", "—Ç—Ä–µ–Ω–¥", "tendencia", "relevancia"]
        }

    def advanced_parse_evaluation_score(self, text: str) -> Dict[str, float]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        scores = {}
        text_lower = text.lower()
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        for criterion, keywords in self.parsing_config.items():
            score = self.extract_score_by_keywords(text_lower, keywords)
            if score is not None:
                scores[criterion] = score
                continue
                
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏)
        structured_scores = self.parse_structured_evaluation(text_lower)
        scores.update(structured_scores)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        if len(scores) < 3:  # –ï—Å–ª–∏ –º–∞–ª–æ –æ—Ü–µ–Ω–æ–∫ –Ω–∞–π–¥–µ–Ω–æ
            sentiment_scores = self.extract_scores_from_sentiment(text_lower)
            scores.update(sentiment_scores)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —É–º–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º
        scores = self.fill_missing_scores(scores, text_lower)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        scores = self.validate_and_normalize_scores(scores)
        
        return scores

    def extract_score_by_keywords(self, text: str, keywords: list) -> float:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        for keyword in keywords:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "–≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å: 8", "engagement 9/10"
            patterns = [
                rf'{keyword}[:\s]*(\d{{1,2}})[\s/]*\d*',  # keyword: 8 –∏–ª–∏ keyword 8/10
                rf'(\d{{1,2}})[\s/]*\d*\s*{keyword}',     # 8 keyword –∏–ª–∏ 8/10 keyword
                rf'{keyword}.*?(\d{{1,2}})/10',           # keyword ... 8/10
                rf'(\d{{1,2}})\s*–∏–∑\s*10.*?{keyword}',    # 8 –∏–∑ 10 ... keyword
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, text)
                if matches:
                    score = float(matches[0])
                    if 1 <= score <= 10:
                        return score
        
        return None

    def parse_structured_evaluation(self, text: str) -> Dict[str, float]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ (—Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏)"""
        scores = {}
        
        # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        table_patterns = [
            r'(\w+)[:\s]*(\d+)/10',  # engagement: 8/10
            r'(\w+)[:\s]*(\d+)\s*–±–∞–ª–ª',  # engagement: 8 –±–∞–ª–ª–æ–≤
            r'(\w+)\s*-\s*(\d+)',    # engagement - 8
        ]
        
        for pattern in table_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                criterion_en, score_str = match
                criterion = self.map_criterion(criterion_en)
                if criterion and score_str.isdigit():
                    score = float(score_str)
                    if 1 <= score <= 10:
                        scores[criterion] = score
        
        return scores

    def extract_scores_from_sentiment(self, text: str) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        scores = {}
        
        # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        sentiment_indicators = {
            "high": ["–æ—Ç–ª–∏—á–Ω–æ", "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ", "–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ", "–≤—ã—Å–æ–∫–∏–π", "—Å–∏–ª—å–Ω—ã–π", "9", "10", "–æ—Ç–ª–∏—á–Ω–∞—è", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–∞—è"],
            "medium": ["—Ö–æ—Ä–æ—à–æ", "–Ω–æ—Ä–º–∞–ª—å–Ω–æ", "—Å—Ä–µ–¥–Ω–∏–π", "—É–º–µ—Ä–µ–Ω–Ω—ã–π", "6", "7", "8", "—Ö–æ—Ä–æ—à–∞—è", "–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è"],
            "low": ["–ø–ª–æ—Ö–æ", "—Å–ª–∞–±–æ", "–Ω–∏–∑–∫–∏–π", "—Å–ª–∞–±—ã–π", "1", "2", "3", "4", "5", "–ø–ª–æ—Ö–∞—è", "—Å–ª–∞–±–∞—è"]
        }
        
        text_words = set(text.lower().split())
        
        for criterion in self.evaluation_criteria:
            # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
            criterion_mentioned = any(
                keyword in text for keyword in self.parsing_config[criterion]
            )
            
            if criterion_mentioned:
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
                if any(word in text_words for word in sentiment_indicators["high"]):
                    scores[criterion] = 8.5
                elif any(word in text_words for word in sentiment_indicators["medium"]):
                    scores[criterion] = 6.5
                elif any(word in text_words for word in sentiment_indicators["low"]):
                    scores[criterion] = 4.0
                else:
                    scores[criterion] = 6.0  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        return scores

    def fill_missing_scores(self, scores: Dict[str, float], text: str) -> Dict[str, float]:
        """–£–º–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫"""
        filled_scores = scores.copy()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –æ—Ü–µ–Ω–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–µ –∫–∞–∫ –±–∞–∑–æ–≤—É—é
        if scores:
            base_score = np.mean(list(scores.values()))
        else:
            # –ê–Ω–∞–ª–∏–∑ –æ–±—â–µ–≥–æ —Ç–æ–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            base_score = self.estimate_base_score_from_text(text)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
        for criterion in self.evaluation_criteria:
            if criterion not in filled_scores:
                # –ù–µ–º–Ω–æ–≥–æ –≤–∞—Ä—å–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                variation = np.random.normal(0, 0.5)  # –Ω–µ–±–æ–ª—å—à–æ–π —Å–ª—É—á–∞–π–Ω—ã–π —Ä–∞–∑–±—Ä–æ—Å
                filled_scores[criterion] = max(1, min(10, base_score + variation))
        
        return filled_scores

    def estimate_base_score_from_text(self, text: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –±–∞–∑–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ —Ç–æ–Ω–∞ —Ç–µ–∫—Å—Ç–∞"""
        positive_words = ["–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—É—Å–ø–µ—à", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω", "–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤"]
        negative_words = ["–ø–ª–æ—Ö–æ", "—Å–ª–∞–±–æ", "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Ä–∏—Å–∫", "–ø—Ä–æ–±–ª–µ–º", "—Å–ª–æ–∂–Ω"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return 7.5
        elif negative_count > positive_count:
            return 4.5
        else:
            return 6.0

    def validate_and_normalize_scores(self, scores: Dict[str, float]) -> Dict[str, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫"""
        validated_scores = {}
        
        for criterion, score in scores.items():
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
            normalized_score = max(1, min(10, score))
            
            # –î–ª—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —à–∫–∞–ª—É
            if criterion == "production_cost" and self.evaluation_criteria[criterion].get("inverted", False):
                normalized_score = 11 - normalized_score  # 1 —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è 10, 10 —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è 1
            
            validated_scores[criterion] = normalized_score
        
        return validated_scores

    def map_criterion(self, criterion_en: str) -> str:
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏"""
        mapping = {
            "engagement": "engagement",
            "involvement": "engagement",
            "viral": "viral_potential", 
            "sharable": "viral_potential",
            "uniqueness": "uniqueness",
            "originality": "uniqueness",
            "production": "production_cost",
            "cost": "production_cost",
            "audience": "audience_fit",
            "trend": "trend_relevance",
            "relevance": "trend_relevance"
        }
        return mapping.get(criterion_en.lower())

    def calculate_confidence_score(self, evaluations: List[IdeaEvaluation]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        confidence_factors = []
        
        for evaluation in evaluations:
            # –§–∞–∫—Ç–æ—Ä 1: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
            found_scores = len([v for v in evaluation.scores.values() if v != 6.0])  # 6.0 - –æ—Ü–µ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            score_completeness = found_scores / len(self.evaluation_criteria)
            
            # –§–∞–∫—Ç–æ—Ä 2: –†–∞–∑–±—Ä–æ—Å –æ—Ü–µ–Ω–æ–∫ –º–µ–∂–¥—É AI (—á–µ–º –±–æ–ª—å—à–µ —Å–æ–≥–ª–∞—Å–∏–µ, —Ç–µ–º –≤—ã—à–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å)
            if len(evaluations) > 1:
                other_scores = [e.final_score for e in evaluations if e != evaluation]
                agreement = 1 - (abs(evaluation.final_score - np.mean(other_scores)) / 10)
            else:
                agreement = 0.7  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ AI
            
            # –§–∞–∫—Ç–æ—Ä 3: –î–ª–∏–Ω–∞ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–¥–±—ç–∫–∞
            feedback_quality = min(1.0, len(evaluation.feedback) / 500)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –¥–ª–∏–Ω–µ
            
            ai_confidence = (score_completeness * 0.4 + agreement * 0.4 + feedback_quality * 0.2)
            confidence_factors.append(ai_confidence)
        
        return np.mean(confidence_factors) if confidence_factors else 0.5

    def calculate_final_score(self, evaluations: List[IdeaEvaluation]) -> Tuple[float, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å"""
        weighted_scores = []
        
        for evaluation in evaluations:
            ai_weight = self.ai_weights.get(evaluation.ai_type, 1.0)
            
            for criterion, config in self.evaluation_criteria.items():
                criterion_score = evaluation.scores.get(criterion, 5.0)
                weighted_scores.append(criterion_score * config["weight"] * ai_weight)
        
        final_score = np.mean(weighted_scores) if weighted_scores else 0
        confidence = self.calculate_confidence_score(evaluations)
        
        return final_score, confidence

    async def evaluate_with_ai(self, idea: str, ai_type: str, prompt_text: str) -> IdeaEvaluation:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏–¥–µ—é —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ AI"""
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∏–¥–µ–µ–π
            formatted_prompt = prompt_text.format(idea=idea)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫—É –æ—Ç AI
            evaluation_text = await analyze_comments_with_prompt(idea, formatted_prompt)
            
            # –ü–∞—Ä—Å–∏–º –æ—Ü–µ–Ω–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
            scores = self.advanced_parse_evaluation_score(evaluation_text)
            
            return IdeaEvaluation(
                idea=idea,
                scores=scores,
                feedback=evaluation_text,
                ai_type=ai_type
            )
        except Exception as e:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ü–µ–Ω–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return IdeaEvaluation(
                idea=idea,
                scores={criterion: 5.0 for criterion in self.evaluation_criteria},
                feedback=f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {str(e)}",
                ai_type=ai_type
            )

    async def optimize_idea_iteration(self, idea: str, iteration: int, prompts: Dict) -> OptimizedIdea:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–¥–µ–∏"""
        print(f"üöÄ –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration} –¥–ª—è –∏–¥–µ–∏: {idea[:50]}...")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–º—è AI
        evaluation_tasks = [
            self.evaluate_with_ai(idea, "creative_ai", prompts['evaluator_creative']),
            self.evaluate_with_ai(idea, "analytical_ai", prompts['evaluator_analytical']),
            self.evaluate_with_ai(idea, "practical_ai", prompts['evaluator_practical'])
        ]
        
        evaluations = await asyncio.gather(*evaluation_tasks)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –±–∞–ª–ª –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
        final_score, confidence = self.calculate_final_score(evaluations)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∏–¥–±—ç–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
        all_feedback = "\n\n".join([
            f"üëÅüó® {eval.ai_type}:\n{eval.feedback}" 
            for eval in evaluations
        ])
        
        # –£–ª—É—á—à–∞–µ–º –∏–¥–µ—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–¥–±—ç–∫–æ–≤
        improvement_prompt = prompts['improver'].format(
            idea=idea,
            feedback=all_feedback,
            iteration=iteration,
            current_score=final_score
        )
        
        optimized_idea_text = await analyze_comments_with_prompt(idea, improvement_prompt)
        
        return OptimizedIdea(
            original_idea=idea,
            optimized_idea=optimized_idea_text,
            iteration=iteration,
            final_score=final_score,
            confidence=confidence,
            evaluation_history=evaluations
        )

    async def run_optimization_pipeline(self, initial_ideas: List[str], prompts: Dict, max_iterations: int = 3) -> List[OptimizedIdea]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π pipeline –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        optimized_ideas = []
        
        for i, idea in enumerate(initial_ideas):
            print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–¥–µ–∏ {i+1}/{len(initial_ideas)}")
            
            current_idea = idea
            best_idea = None
            best_score = 0
            
            for iteration in range(1, max_iterations + 1):
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –Ω–∞ —Ç–µ–∫—É—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                optimized = await self.optimize_idea_iteration(current_idea, iteration, prompts)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é
                if optimized.final_score > best_score:
                    best_idea = optimized
                    best_score = optimized.final_score
                
                # –î–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                current_idea = optimized.optimized_idea
                
                print(f"  ‚úÖ –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: –æ—Ü–µ–Ω–∫–∞ {optimized.final_score:.2f}, –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å {optimized.confidence:.2f}")
            
            if best_idea:
                optimized_ideas.append(best_idea)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É –±–∞–ª–ª—É
        optimized_ideas.sort(key=lambda x: x.final_score, reverse=True)
        
        return optimized_ideas

    def generate_optimization_report(self, optimized_ideas: List[OptimizedIdea]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        report = "üß† <b>–û–¢–ß–ï–¢ –ü–û –ò–¢–ï–†–ê–¢–ò–í–ù–û–ô –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ò–î–ï–ô</b>\n\n"
        report += f"üìä –í—Å–µ–≥–æ –∏–¥–µ–π: {len(optimized_ideas)}\n"
        report += f"üïê –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n"
        
        for i, idea in enumerate(optimized_ideas[:5], 1):  # –¢–æ–ø-5 –∏–¥–µ–π
            report += f"üèÜ <b>–ò–î–ï–Ø #{i} (–û—Ü–µ–Ω–∫–∞: {idea.final_score:.2f}/10, –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {idea.confidence*100:.1f}%)</b>\n"
            report += f"üîÑ –ò—Ç–µ—Ä–∞—Ü–∏–π: {idea.iteration}\n\n"
            
            report += f"üí° <b>–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è:</b>\n{idea.optimized_idea}\n\n"
            
            # –î–µ—Ç–∞–ª–∏ –æ—Ü–µ–Ω–æ–∫
            report += "üìà <b>–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:</b>\n"
            for criterion, config in self.evaluation_criteria.items():
                avg_score = np.mean([eval.scores.get(criterion, 5) for eval in idea.evaluation_history])
                report += f"  ‚Ä¢ {criterion}: {avg_score:.1f}/10 (–≤–µ—Å: {config['weight']*100}%)\n"
            
            report += "\n" + "="*50 + "\n\n"
        
        return report


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
optimizer = AdvancedIterativeIdeasOptimizer()